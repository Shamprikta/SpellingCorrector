{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "course_slug": "python-text-mining",
      "graded_item_id": "r35En",
      "launcher_item_id": "tCVfW",
      "part_id": "NTVgL"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "Copy of text_spelling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rC6qvVQnib3"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "text = pd.read_csv('testdata.txt', sep='\\t', delimiter= None, header = None, error_bad_lines= False)\n",
        "dictionary = open('vocab.txt', 'r')\n",
        "content = dictionary.read()\n",
        "rows = content.split('\\n')\n",
        "\n",
        "\n",
        "#print(content)\n",
        "#print(text)\n",
        "#print(type(text))\n",
        "#dic = pd.read_csv('vocab.txt', header = None, error_bad_lines= False)\n",
        "#dic_list = dic[0].values.tolist()\n",
        "#print(type(dic_list))\n",
        "#print(dic.head(100))\n",
        "#print(type(dic_list[22344]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F88VWVTFnib_"
      },
      "source": [
        "from nltk.corpus import words\n",
        "from nltk import edit_distance\n",
        "import sys\n",
        "correct_spellings = words.words()\n",
        "\n",
        "from nltk import ngrams\n",
        "from nltk import jaccard_distance\n",
        "\n",
        "\n",
        "def correct_a_word(uncorrect_word):\n",
        "    proposal = uncorrect_word\n",
        "    dist = 1.0\n",
        "    \n",
        "    if len(uncorrect_word) > 2:\n",
        "        ngram_uncorrect_word = set(ngrams(uncorrect_word,3))\n",
        "    else:\n",
        "        ngram_uncorrect_word = set(ngrams(uncorrect_word,1))\n",
        "    \n",
        "    for correct_word in rows:\n",
        "        if isinstance(correct_word, float) or isinstance(correct_word, int):\n",
        "            return proposal\n",
        "        else:\n",
        "            if (correct_word.startswith(uncorrect_word[0])):\n",
        "                if len(correct_word)>2:\n",
        "                    ngram = set(ngrams(correct_word,3))\n",
        "                    d = jaccard_distance(ngram_uncorrect_word, ngram)\n",
        "                    #print(correct_word)\n",
        "                    #print(d)\n",
        "                    if dist>d:\n",
        "                        # print(d)\n",
        "                        proposal = correct_word\n",
        "                        dist = d        \n",
        "                else:\n",
        "                    ngram = set(ngrams(correct_word,1))\n",
        "                    d = jaccard_distance(ngram_uncorrect_word, ngram)\n",
        "#                     print(correct_word)\n",
        "#                     print(d)\n",
        "                    if dist>d:\n",
        "                        #print(d)\n",
        "                        proposal = correct_word\n",
        "                        dist = d\n",
        "    return proposal\n",
        "\n",
        "\n",
        "def compare_2_words(uncorrect_word, proposed_word1, proposed_word2, proposed_word3):\n",
        "    \n",
        "    proposal = uncorrect_word\n",
        "    #dist = 1.0\n",
        "    dist = sys.float_info.max\n",
        "    words = [proposed_word1, proposed_word2, proposed_word3]\n",
        "    \n",
        "#     proposal = uncorrect_word\n",
        "#     dist = sys.float_info.max\n",
        "    \n",
        "#     for s in words:\n",
        "#         if (s.startswith(uncorrect_word[0])):\n",
        "#             d = edit_distance(uncorrect_word,s,transpositions=True)\n",
        "#             print(correct_word)\n",
        "#             print(d)\n",
        "#             if (dist>d):\n",
        "#                     proposal = correct_word\n",
        "#                     dist = d\n",
        "#     return proposal\n",
        "    \n",
        "    #if 3 of the words are the same we will return one of them since they are the same\n",
        "    if proposed_word1 == proposed_word2 and proposed_word2 == proposed_word3:\n",
        "        return proposed_word1\n",
        "    #if two words are the same then return one of them\n",
        "    elif proposed_word1 == proposed_word2:\n",
        "        return proposed_word1\n",
        "    elif proposed_word2 == proposed_word3:\n",
        "        return proposed_word2\n",
        "    elif proposed_word1 == proposed_word3:\n",
        "        return proposed_word1\n",
        "    elif proposed_word1 != proposed_word2 and proposed_word2 != proposed_word3:\n",
        "        print('Here')\n",
        "        # if all of the 3 words are different\n",
        "#         if len(uncorrect_word) > 2:\n",
        "#             ngram_uncorrect_word = set(ngrams(uncorrect_word,3))\n",
        "#         else:\n",
        "#             ngram_uncorrect_word = set(ngrams(uncorrect_word,1))\n",
        "            \n",
        "        for s in words:\n",
        "            if (s.startswith(uncorrect_word[0])):\n",
        "#                 if len(s)>2 :\n",
        "#                     ngram = set(ngrams(s,3))\n",
        "#                 else:\n",
        "#                     ngram = set(ngrams(s,1))\n",
        "                #d = jaccard_distance(ngram_uncorrect_word, ngram)\n",
        "                d = edit_distance(uncorrect_word,s,transpositions=True)\n",
        "                #print('distance' + str(d))\n",
        "                if dist>d:\n",
        "                    #print('distance' + str(dist))\n",
        "                    dist = d\n",
        "                    proposal = s\n",
        "\n",
        "#         if (proposed_word2.startswith(uncorrect_word[0])):\n",
        "#             if len(proposed_word2)>2:\n",
        "#                 ngram = set(ngrams(proposed_word2,3))\n",
        "#             else:\n",
        "#                 ngram = set(ngrams(proposed_word2,1))\n",
        "#             d = jaccard_distance(ngram_uncorrect_word, ngram)\n",
        "#             if dist>d:\n",
        "#                 dist = d\n",
        "#                 proposal = proposed_word2\n",
        "\n",
        "#         if (proposed_word3.startswith(uncorrect_word[0])):\n",
        "#             if len(proposed_word3)>2:\n",
        "#                 ngram = set(ngrams(proposed_word3,3))\n",
        "#             else:\n",
        "#                 ngram = set(ngrams(proposed_word3,1))\n",
        "#             d = jaccard_distance(ngram_uncorrect_word, ngram)\n",
        "#             if dist>d:\n",
        "#                 dist = d\n",
        "#                 proposal = proposed_word3\n",
        "                \n",
        "#         if (proposed_word1.startswith(uncorrect_word[0])):\n",
        "#             if len(proposed_word1)>2:\n",
        "#                 ngram = set(ngrams(proposed_word1,3))\n",
        "#             else:\n",
        "#                 ngram = set(ngrams(proposed_word1,1))\n",
        "#             d = jaccard_distance(ngram_uncorrect_word, ngram)\n",
        "#             if dist>d:\n",
        "#                 dist = d\n",
        "#                 proposal = proposed_word1\n",
        "    \n",
        "    return proposal\n",
        "\n",
        "# def double_check(uncorrect_word, proposed_word1, proposed_word2, proposed_word3):\n",
        "#     proposal = uncorrect_word\n",
        "#     dist = sys.float_info.max\n",
        "    \n",
        "#     #for correct_word in rows:\n",
        "#         if isinstance(correct_word, float) or isinstance(correct_word, int):\n",
        "#             break\n",
        "#         else:\n",
        "#             if (correct_word.startswith(uncorrect_word[0])):\n",
        "#                 d = edit_distance(uncorrect_word,correct_word,transpositions=True)\n",
        "#                 if (dist>d):\n",
        "#                         proposal = correct_word\n",
        "#                         dist = d\n",
        "#     return proposal\n",
        "\n",
        "\n",
        "\n",
        "def correct2_a_word(uncorrect_word):\n",
        "    proposal = uncorrect_word\n",
        "    dist = sys.float_info.max\n",
        "    \n",
        "    for correct_word in rows:\n",
        "        if isinstance(correct_word, float) or isinstance(correct_word, int):\n",
        "            break\n",
        "        else:\n",
        "            if (correct_word.startswith(uncorrect_word[0])):\n",
        "                d = edit_distance(uncorrect_word,correct_word,transpositions=True)\n",
        "                #print(correct_word)\n",
        "                #print(d)\n",
        "                if (dist>d):\n",
        "                        proposal = correct_word\n",
        "                        dist = d\n",
        "    return proposal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XP5OEAgnicG"
      },
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def words(text): \n",
        "    return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "WORDS = Counter(words(open('vocab.txt').read()))\n",
        "\n",
        "def P(word, N=sum(WORDS.values())): \n",
        "    #Probability of `word`.\"\n",
        "    return WORDS[word] / N\n",
        "\n",
        "def correction(word): \n",
        "    #\"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word): \n",
        "    #\"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words): \n",
        "    #\"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    #\"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word): \n",
        "    #\"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "vCoJqjUbnicL"
      },
      "source": [
        "import nltk\n",
        "import string\n",
        "\n",
        "corrected_word_index = []\n",
        "corrected_words1 = []\n",
        "corrected_words2 = []\n",
        "corrected_words3 = []\n",
        "final_words = []\n",
        "\n",
        "result = open('result.txt', 'w+')\n",
        "for j in range(len(text)):\n",
        "    \n",
        "    corrected_word_index.clear()\n",
        "    corrected_words1.clear()\n",
        "    corrected_words2.clear()\n",
        "    corrected_words3.clear()\n",
        "    final_words.clear()\n",
        "    \n",
        "    #n_error = text[1][j]\n",
        "    sent = text[2][j]\n",
        "    word_tokens = nltk.word_tokenize(sent)\n",
        "\n",
        "    for i in range(len(word_tokens)):\n",
        "#         if n_error == 0:\n",
        "#             break\n",
        "#         else:\n",
        "            correct_word = correct2_a_word(word_tokens[i])\n",
        "            if correct_word != str(word_tokens[i]):\n",
        "                #n_error -= 1;\n",
        "                corrected_word_index.append(i)\n",
        "                corrected_words1.append(correct_word)\n",
        "                \n",
        "    for i in range(len(corrected_word_index)):\n",
        "        corrected_words2.append(correction(word_tokens[corrected_word_index[i]]))\n",
        "        corrected_words3.append(correct2_a_word(word_tokens[corrected_word_index[i]]))\n",
        "        \n",
        "    for i in range(len(corrected_word_index)):\n",
        "        #final_words.append(compare_2_words(word_tokens[corrected_word_index[i]], corrected_words1[i], corrected_words2[i], corrected_words3[i]))\n",
        "        word_tokens[corrected_word_index[i]] = compare_2_words(word_tokens[corrected_word_index[i]], corrected_words1[i], corrected_words2[i], corrected_words3[i])\n",
        "        #print(word_tokens[corrected_word_index[i]])\n",
        "    \n",
        "    sentence = \"\".join([\" \"+i if not i.startswith(\"'\") and i not in string.punctuation else i for i in word_tokens]).strip()\n",
        "    result.write(str(j+1) + '\\t' + sentence + '\\n')\n",
        "\n",
        "result.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrT5wvY7nicS",
        "outputId": "bd9e2061-5e0a-46cc-f97d-7a427d695660"
      },
      "source": [
        "#corr_word = correct_a_word('protectionst')\n",
        "corr_word = correct_a_word('whoe')\n",
        "corr_word1 = correction('whoe')\n",
        "corr_word2 = correct2_a_word('whoe')\n",
        "compare = compare_2_words('Taawin', 'Twins', 'alwin', 'Taiwan')\n",
        "print('correct_a_word: ' + corr_word)\n",
        "print('correction: ' + corr_word1)\n",
        "print('correct2_a_word: ' + corr_word2)\n",
        "print('compare: ' + compare)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Here\n",
            "correct_a_word: whoe\n",
            "correction: whoe\n",
            "correct2_a_word: whoe\n",
            "compare: Taiwan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVjgnrqbnicZ",
        "outputId": "7a4acd95-90c9-455d-8a0d-dce9c3d76ccd"
      },
      "source": [
        "\n",
        "n_error = text[1][49]\n",
        "sent = text[2][49]\n",
        "print(text[2][49])\n",
        "word_tokens = nltk.word_tokenize(sent)\n",
        "print(word_tokens)\n",
        "print(type(word_tokens))\n",
        "word_tokens[11] = 'ASDSDSAD'\n",
        "print(word_tokens)\n",
        "sentence = \"\".join([\" \"+i if not i.startswith(\"'\") and i not in string.punctuation else i for i in word_tokens]).strip()\n",
        "print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Indonesian exports of CPO in calendar 1986 were 530,500  tonnes, aiagnst 468,500 in 1985, according to central bank  figures.  \n",
            "['Indonesian', 'exports', 'of', 'CPO', 'in', 'calendar', '1986', 'were', '530,500', 'tonnes', ',', 'aiagnst', '468,500', 'in', '1985', ',', 'according', 'to', 'central', 'bank', 'figures', '.']\n",
            "<class 'list'>\n",
            "['Indonesian', 'exports', 'of', 'CPO', 'in', 'calendar', '1986', 'were', '530,500', 'tonnes', ',', 'ASDSDSAD', '468,500', 'in', '1985', ',', 'according', 'to', 'central', 'bank', 'figures', '.']\n",
            "Indonesian exports of CPO in calendar 1986 were 530,500 tonnes, ASDSDSAD 468,500 in 1985, according to central bank figures.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAGp2_Ganice",
        "outputId": "a2064795-c65d-4548-8326-fdbd7995b215"
      },
      "source": [
        "import nltk\n",
        "anspath='dataset_text_spelling/ans.txt'\n",
        "resultpath='dataset_text_spelling/result.txt'\n",
        "\n",
        "\n",
        "# Evaluate the result\n",
        "def eval():\n",
        "    ansfile=open(anspath,'r')\n",
        "    resultfile=open(resultpath,'r')\n",
        "    count=0\n",
        "    nb_line=0\n",
        "    for r, a in zip(resultfile,ansfile):\n",
        "        ansline=a.split('\\t')[1]\n",
        "        ansset=set(nltk.word_tokenize(ansline))\n",
        "        resultline=r.split('\\t')[1]\n",
        "        resultset=set(nltk.word_tokenize(resultline))\n",
        "        if ansset==resultset:\n",
        "            count+=1\n",
        "        nb_line+=1\n",
        "    print(\"Accuracy is : %.2f%%\" % (count*1.00/nb_line))\n",
        "\n",
        "eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is : 0.67%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ux8LCJ2ynicm",
        "outputId": "cd465484-ec7f-45ba-90f6-ccffead93db9"
      },
      "source": [
        "# for i in range(len(word_tokens)):\n",
        "# #     if n_error == 0:\n",
        "# #             break\n",
        "# #     else:\n",
        "#     correct_word = correct_a_word(word_tokens[i])\n",
        "#     print(correct_word)\n",
        "#         #print(type(word_tokens[i]))\n",
        "#     if correct_word != str(word_tokens[i]):\n",
        "#         n_error -= 1;\n",
        "#         corrected_word_index.append(i)\n",
        "#         corrected_words1.append(correct_word)\n",
        "\n",
        "result = open('dataset_text_spelling/result.txt'\n",
        "              , 'w+')\n",
        "sentence_tokens = nltk.sent_tokenize(text[2][0])\n",
        "# punctuation = '.,;?!'\n",
        "# for symbol in punctuation:\n",
        "#     sent = sent.replace(symbol, '')\n",
        "#print(sent)n_error = text[1][49]\n",
        "sent = text[2][49]\n",
        "print(text[2][49])\n",
        "word_tokens = nltk.word_tokenize(sent)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #print('dist2: ' + str(dist2))\n",
        "#     if dist1 > dist2:\n",
        "#         proposal = proposed_word2\n",
        "#     elif dist2 > dist1:\n",
        "#         proposal = proposed_word1\n",
        "#     else:\n",
        "#         if proposed_word1.startswith(uncorrect_word[0]):\n",
        "#             proposal = proposed_word1\n",
        "#         else:\n",
        "#             proposal = proposed_word2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Indonesian exports of CPO in calendar 1986 were 530,500  tonnes, aiagnst 468,500 in 1985, according to central bank  figures.  \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}